#!/bin/bash
#
# generate_cluster_yaml.sh
# CSV 파일에서 서버 정보를 읽고, SSH로 hostname을 조회하여 클러스터 YAML 파일을 생성
#
# 사용법:
#   ./generate_cluster_yaml.sh <input.csv> [output.yaml]
#
# CSV 형식:
#   ip,role,ssh_user,ssh_password
#   192.168.1.1,controller,root,password123
#   192.168.1.2,compute,root,password456
#   192.168.1.3,viz,root,password789
#
# role 값: controller, compute, viz
#

set -e

# 색상 정의
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# sshpass 설치 확인
check_sshpass() {
    if ! command -v sshpass &> /dev/null; then
        echo -e "${RED}[ERROR]${NC} sshpass가 설치되어 있지 않습니다."
        echo "설치 명령어: sudo apt install sshpass"
        exit 1
    fi
}

# 사용법 출력
usage() {
    echo "사용법: $0 <input.csv> [output.yaml]"
    echo ""
    echo "CSV 형식:"
    echo "  ip,role,ssh_user,ssh_password"
    echo "  192.168.1.1,controller,root,password123"
    echo "  192.168.1.2,compute,root,password456"
    echo "  192.168.1.3,viz,root,password789"
    echo ""
    echo "role 값: controller, compute, viz"
    exit 1
}

# SSH를 통해 서버 정보 조회
get_server_info() {
    local ip="$1"
    local user="$2"
    local password="$3"
    local timeout=10

    # SSH 옵션 (-n prevents SSH from reading stdin)
    local ssh_opts="-n -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -o ConnectTimeout=$timeout -o LogLevel=ERROR"

    # hostname 조회
    local hostname
    hostname=$(sshpass -p "$password" ssh $ssh_opts "${user}@${ip}" "hostname" 2>/dev/null) || hostname=""

    if [ -z "$hostname" ]; then
        echo -e "${YELLOW}[WARN]${NC} $ip: hostname 조회 실패, IP 기반 이름 사용" >&2
        hostname="node-${ip//./-}"
    fi

    # CPU 정보 조회
    local cpus sockets cores_per_socket threads_per_core
    cpus=$(sshpass -p "$password" ssh $ssh_opts "${user}@${ip}" "nproc" 2>/dev/null)
    cpus=${cpus:-2}
    sockets=$(sshpass -p "$password" ssh $ssh_opts "${user}@${ip}" "lscpu | grep 'Socket(s):' | awk '{print \$2}'" 2>/dev/null)
    sockets=${sockets:-1}
    cores_per_socket=$(sshpass -p "$password" ssh $ssh_opts "${user}@${ip}" "lscpu | grep 'Core(s) per socket:' | awk '{print \$4}'" 2>/dev/null)
    cores_per_socket=${cores_per_socket:-$cpus}
    threads_per_core=$(sshpass -p "$password" ssh $ssh_opts "${user}@${ip}" "lscpu | grep 'Thread(s) per core:' | awk '{print \$4}'" 2>/dev/null)
    threads_per_core=${threads_per_core:-1}

    # 메모리 조회 (MB) - total에서 10GB 여유 메모리 제외
    local memory_mb
    memory_mb=$(sshpass -p "$password" ssh $ssh_opts "${user}@${ip}" "LC_ALL=C free -m | awk '/^Mem:/{reserved=10240; total=\$2; available=total-reserved; if(available<1024) available=int(total*0.9); else available=int(available); print available}'" 2>/dev/null)
    memory_mb=${memory_mb:-4096}

    # 디스크 조회 (GB)
    local disk_gb
    disk_gb=$(sshpass -p "$password" ssh $ssh_opts "${user}@${ip}" "df -BG / | awk 'NR==2{gsub(/G/,\"\",\$2); print \$2}'" 2>/dev/null)
    disk_gb=${disk_gb:-100}

    # GPU 정보 조회
    local has_nvidia_gpu=false
    local has_amd_gpu=false
    local gpu_count=0

    # NVIDIA GPU 체크
    if sshpass -p "$password" ssh $ssh_opts "${user}@${ip}" "command -v nvidia-smi &>/dev/null && nvidia-smi -L" 2>/dev/null | grep -q "GPU"; then
        has_nvidia_gpu=true
        gpu_count=$(sshpass -p "$password" ssh $ssh_opts "${user}@${ip}" "nvidia-smi -L 2>/dev/null | wc -l" 2>/dev/null) || gpu_count=1
    fi

    # AMD GPU 체크
    if sshpass -p "$password" ssh $ssh_opts "${user}@${ip}" "lspci | grep -i 'VGA.*AMD\|Display.*AMD'" 2>/dev/null | grep -qi "amd"; then
        has_amd_gpu=true
        gpu_count=$(sshpass -p "$password" ssh $ssh_opts "${user}@${ip}" "lspci | grep -ic 'VGA.*AMD\|Display.*AMD'" 2>/dev/null) || gpu_count=1
    fi

    # 결과 출력 (파이프 구분자 사용)
    echo "${hostname}|${cpus}|${sockets}|${cores_per_socket}|${threads_per_core}|${memory_mb}|${disk_gb}|${has_nvidia_gpu}|${has_amd_gpu}|${gpu_count}"
}

# YAML 헤더 생성
generate_yaml_header() {
    local output_file="$1"
    local ssh_password="$2"

    cat > "$output_file" << 'EOF'
# Multi-Head HPC Cluster Configuration
# Purpose: Full-Copy Quad-Redundancy with Dynamic Clustering
# Version: 2.1
# Generated by: generate_cluster_yaml.sh
#
# 사용 방법:
#   1. 이 파일을 편집하여 모든 설정을 구성합니다
#   2. environment 섹션에서 비밀번호 설정
#   3. 환경변수 설정 불필요! YAML만으로 완전 자동화
#   4. sudo ./setup_cluster_full_multihead.sh 실행
#
# 보안 주의사항:
#   - 민감 정보가 포함되므로 파일 권한 설정: chmod 600 my_multihead_cluster.yaml
#   - Git 저장소에 커밋 시 주의 (.gitignore 권장)

config_version: '2.1'
stage: 0  # Phase 0부터 시작

cluster_info:
  cluster_name: smart-twin-cluster
  domain: hpc.local
  admin_email: admin@example.com
  timezone: Asia/Seoul
EOF
    echo "  ssh_password: \"$ssh_password\"  # 모든 노드 공통 SSH 비밀번호" >> "$output_file"
    echo "" >> "$output_file"
}

# Controller 노드 YAML 생성
generate_controller_yaml() {
    local hostname="$1"
    local ip="$2"
    local ssh_user="$3"
    local cpus="$4"
    local memory_mb="$5"
    local disk_gb="$6"
    local priority="$7"
    local is_primary="$8"

    local vip_owner="false"
    if [ "$is_primary" = "true" ]; then
        vip_owner="true"
    fi

    cat << EOF
    - hostname: $hostname
      ip_address: $ip
      ssh_user: $ssh_user
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: controller
      priority: $priority

      services:
        glusterfs: true
        mariadb: true
        redis: true
        slurm: true
        web: true
        keepalived: true

      vip_owner: $vip_owner

      hardware:
        cpus: $cpus
        memory_mb: $memory_mb
        disk_gb: $disk_gb

EOF
}

# Compute 노드 YAML 생성
generate_compute_yaml() {
    local hostname="$1"
    local ip="$2"
    local ssh_user="$3"
    local cpus="$4"
    local sockets="$5"
    local cores_per_socket="$6"
    local threads_per_core="$7"
    local memory_mb="$8"
    local disk_gb="$9"

    cat << EOF
    - hostname: $hostname
      ip_address: $ip
      ssh_user: $ssh_user
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: compute
      hardware:
        cpus: $cpus
        sockets: $sockets
        cores_per_socket: $cores_per_socket
        threads_per_core: $threads_per_core
        memory_mb: $memory_mb
        tmp_disk_mb: $((disk_gb * 1000 / 2))

EOF
}

# Viz 노드 YAML 생성
generate_viz_yaml() {
    local hostname="$1"
    local ip="$2"
    local ssh_user="$3"
    local cpus="$4"
    local sockets="$5"
    local cores_per_socket="$6"
    local threads_per_core="$7"
    local memory_mb="$8"
    local disk_gb="$9"
    local has_nvidia="${10}"
    local has_amd="${11}"
    local gpu_count="${12}"

    local gpu_type="nvidia"
    if [ "$has_amd" = "true" ]; then
        gpu_type="amd"
    fi

    cat << EOF
    - hostname: $hostname
      ip_address: $ip
      ssh_user: $ssh_user
      ssh_port: 22
      ssh_key_path: ~/.ssh/id_rsa
      os_type: ubuntu22
      node_type: viz
      hardware:
        cpus: $cpus
        sockets: $sockets
        cores_per_socket: $cores_per_socket
        threads_per_core: $threads_per_core
        memory_mb: $memory_mb
        tmp_disk_mb: $((disk_gb * 1000 / 2))
        gpus: $gpu_count
        gpu_type: $gpu_type
        gres: gpu:${gpu_type}:${gpu_count}

EOF
}

# YAML 푸터 (나머지 설정) 생성
generate_yaml_footer() {
    local output_file="$1"
    local controller_ips="$2"
    local compute_nodes="$3"
    local viz_nodes="$4"

    # 첫 번째 컨트롤러 IP의 네트워크 추출
    local first_ip
    first_ip=$(echo "$controller_ips" | head -1)
    local network_prefix
    network_prefix=$(echo "$first_ip" | cut -d'.' -f1-3)

    cat >> "$output_file" << EOF
# ============================================================================
# Virtual IP (VIP) Configuration
# ============================================================================
network:
  management_network: ${network_prefix}.0/24
  compute_network: ${network_prefix}.0/24

  vip:
    address: ${network_prefix}.100
    netmask: 24
    interface: ens18
    vrrp_router_id: 51
    auth_password: hpc_cluster_vip_secret

  firewall:
    enabled: true
    ports:
      slurmd: 6818
      slurmctld: 6817
      slurmdbd: 6819
      auth_backend: 4430
      auth_frontend: 4431
      dashboard_backend: 5010
      websocket: 5011
      cae_backend: 5000
      cae_automation: 5001
      dashboard_frontend: 5173
      app_frontend: 5174
      mariadb: 3306
      redis: 6379
      glusterd: 24007
      glusterd_mgmt: 24008
      gluster_brick_start: 49152
      gluster_brick_end: 49156
      prometheus: 9090
      node_exporter: 9100
      mariadb_exporter: 9104
      redis_exporter: 9121
      ssh: 22
      http: 80
      https: 443
      vrrp: 112

# ============================================================================
# Shared Storage Configuration (GlusterFS)
# ============================================================================
shared_storage:
  type: glusterfs

  glusterfs:
    volume_name: shared_data
    volume_type: replica
    replica_count: auto
    brick_path: /data/glusterfs/shared
    mount_point: /mnt/gluster
    options:
      performance.cache-size: 256MB
      network.ping-timeout: 10
      cluster.self-heal-daemon: enable
    directories:
      - frontend_builds
      - slurm/state
      - slurm/logs
      - slurm/spool
      - uploads
      - config

  backup:
    enabled: true
    path: /data/system_backup
    schedule:
      full_backup: "0 2 * * *"
      incremental: "0 * * * *"
    retention:
      daily: 7
      weekly: 4
      monthly: 12
    items:
      - configs
      - databases
      - slurm_state
      - glusterfs_meta

# ============================================================================
# Database Configuration (MariaDB Galera Cluster)
# ============================================================================
database:
  enabled: true
  type: mariadb-galera

  mariadb:
    version: "10.6"
    galera:
      cluster_name: hpc_portal_cluster
      sst_method: rsync
      cluster_address: auto
      slave_threads: 4
    databases:
      - name: slurm_acct_db
        user: slurm
        password: "\${DB_SLURM_PASSWORD}"
      - name: auth_portal
        user: auth_user
        password: "\${DB_AUTH_PASSWORD}"
    config:
      max_connections: 500
      innodb_buffer_pool_size: 8G
      innodb_log_file_size: 512M

# ============================================================================
# Redis Cluster Configuration
# ============================================================================
redis:
  enabled: true
  type: cluster
  cluster:
    port: 6379
    password: "\${REDIS_PASSWORD}"
    cluster_mode: true
    replicas: 0
    nodes: auto
    cluster_node_timeout: 5000
    appendonly: yes
    appendfsync: everysec
    maxmemory: 4gb
    maxmemory_policy: allkeys-lru

# ============================================================================
# Slurm Configuration
# ============================================================================
slurm_config:
  version: 23.02.7
  install_path: /usr/local/slurm
  config_path: /usr/local/slurm/etc
  log_path: /mnt/gluster/slurm/logs
  spool_path: /var/spool/slurm
  state_save_location: /mnt/gluster/slurm/state

  multi_master:
    enabled: true
    controllers: auto

  accounting:
    storage_type: accounting_storage/slurmdbd
    storage_host: 127.0.0.1
    storage_port: 6819

  scheduler:
    type: sched/backfill
    max_job_count: 10000
    max_array_size: 1000

  partitions:
EOF

    # Compute 파티션 생성
    if [ -n "$compute_nodes" ]; then
        local compute_count
        compute_count=$(echo "$compute_nodes" | wc -l)
        cat >> "$output_file" << EOF
    - name: normal
      nodes: node[001-$(printf "%03d" $compute_count)]
      default: true
      max_time: 7-00:00:00
      max_nodes: $compute_count
      state: UP
EOF
    fi

    # Viz 파티션 생성
    if [ -n "$viz_nodes" ]; then
        local viz_count
        viz_count=$(echo "$viz_nodes" | wc -l)
        cat >> "$output_file" << EOF

    - name: viz
      nodes: viz-node[001-$(printf "%03d" $viz_count)]
      default: false
      max_time: "60-00:00:00"
      max_nodes: $viz_count
      state: UP
      gres: gpu:1
      exclusive: false
EOF
    fi

    cat >> "$output_file" << 'EOF'

# ============================================================================
# Web Services Configuration
# ============================================================================
web_services:
  enabled: true
  shared_config_path: /mnt/gluster/config
  frontend_build_path: /mnt/gluster/frontend_builds
  services:
    - name: auth_portal
      port: 4430
      type: backend
      health_check: /health
    - name: auth_frontend
      port: 4431
      type: frontend
      build_path: auth_frontend_4431
    - name: dashboard_backend
      port: 5010
      type: backend
      health_check: /health
    - name: websocket
      port: 5011
      type: backend
      websocket: true
    - name: cae_backend
      port: 5000
      type: backend
      health_check: /health
    - name: cae_automation
      port: 5001
      type: backend
      health_check: /health
    - name: dashboard_frontend
      port: 5173
      type: frontend
      build_path: dashboard_5173
    - name: app_frontend
      port: 5174
      type: frontend
      build_path: app_5174

# ============================================================================
# High Availability Configuration
# ============================================================================
high_availability:
  keepalived:
    enabled: true
    health_check:
      script: /usr/local/bin/check_all_services.sh
      interval: 2
      weight: -20
      fall: 3
      rise: 2
    notifications:
      master: /usr/local/bin/notify_master.sh
      backup: /usr/local/bin/notify_backup.sh
      fault: /usr/local/bin/notify_fault.sh

  auto_recovery:
    enabled: true
    galera:
      enabled: true
      script: /opt/hpc-dashboard/cluster/utils/galera_auto_recover.sh
    redis:
      enabled: true
      rebalance_on_join: true

# ============================================================================
# Monitoring & Alerting
# ============================================================================
monitoring:
  prometheus:
    enabled: true
    port: 9090
    retention_days: 30
    scrape_interval: 15s
  node_exporter:
    enabled: true
    port: 9100
  mariadb_exporter:
    enabled: true
    port: 9104
  redis_exporter:
    enabled: true
    port: 9121
  grafana:
    enabled: true
    port: 3000
    admin_password: "${GRAFANA_PASSWORD}"
  alerting:
    enabled: true
    channels:
      - type: slack
        webhook_url: "${SLACK_WEBHOOK_URL}"
        channel: "#hpc-alerts"
      - type: email
        smtp_host: smtp.gmail.com
        smtp_port: 587
        from: alerts@hpc.local
        to: admin@hpc.local
    rules:
      - alert: NodeDown
        duration: 30s
        severity: critical
      - alert: HighCPU
        threshold: 90
        duration: 5m
        severity: warning
      - alert: HighDisk
        threshold: 80
        duration: 1m
        severity: warning
      - alert: GaleraNodeDown
        duration: 10s
        severity: critical
      - alert: VIPFailover
        duration: 0s
        severity: info

# ============================================================================
# User & Security Configuration
# ============================================================================
users:
  admin_user: admin
  slurm_user: slurm
  slurm_uid: 1001
  slurm_gid: 1001
  munge_user: munge
  munge_uid: 1002
  munge_gid: 1002
  cluster_users:
    - username: user01
      uid: 2001
      gid: 2001
      groups: [users, hpc]

security:
  munge:
    key_rotation_days: 30
  selinux:
    enabled: true
    mode: permissive
  jwt:
    secret_key: "${JWT_SECRET_KEY}"
    algorithm: HS256
    expiration_hours: 8

# ============================================================================
# Container & GPU Support
# ============================================================================
container_support:
  apptainer:
    enabled: true
    version: 1.2.5
    install_path: /usr/local
    image_path: /share/apptainer/images
    cache_path: /tmp/apptainer
    bind_paths: [/home, /share, /scratch, /tmp]
    deployment_strategy: local_copy
    master_images_path: /scratch/apptainers
    node_local_path: /scratch/apptainers

gpu_computing:
  nvidia:
    enabled: true
    nodes: []
  amd:
    enabled: true
    nodes: []

# ============================================================================
# Dynamic Clustering Options
# ============================================================================
clustering:
  auto_discovery:
    enabled: true
    timeout: 5
    health_check_interval: 30
  auto_join:
    enabled: true
  node_management:
    allow_dynamic_add: true
    allow_dynamic_remove: true
    min_controllers: 1
    max_controllers: 10
  status_format: table

# ============================================================================
# Environment Variables (Template)
# ============================================================================
environment:
  DB_SLURM_PASSWORD: "slurm_secure_password"
  DB_AUTH_PASSWORD: "auth_secure_password"
  REDIS_PASSWORD: "redis_cluster_secret"
  JWT_SECRET_KEY: "jwt_super_secret_key_change_me"
  GRAFANA_PASSWORD: "grafana_admin_password"
  SLACK_WEBHOOK_URL: "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"

# ============================================================================
# Installation & Deployment
# ============================================================================
installation:
  install_method: package
  offline_mode: false
  package_cache_path: /var/cache/cluster_packages
  auto_confirm: false
  skip_munge: false
  skip_slurm_base: false
  install_mpi: false
  install_apptainer: false
  timeouts:
    cluster_join_wait: 300
    service_start_wait: 30
    ssh_timeout: 10
    health_check_interval: 10
  phases:
    phase0_storage: true
    phase1_database: true
    phase2_redis: true
    phase3_slurm: true
    phase4_keepalived: true
    phase5_web: true
    phase6_monitoring: true

# ============================================================================
# Time Synchronization
# ============================================================================
time_synchronization:
  enabled: true
  ntp_servers:
    - time.google.com
    - time.cloudflare.com
    - pool.ntp.org
  timezone: Asia/Seoul
EOF
}

# 메인 함수
main() {
    local input_csv="$1"
    local output_yaml="${2:-cluster_config.yaml}"

    # 입력 파일 확인
    if [ ! -f "$input_csv" ]; then
        echo -e "${RED}[ERROR]${NC} CSV 파일을 찾을 수 없습니다: $input_csv"
        exit 1
    fi

    check_sshpass

    echo -e "${BLUE}=================================================${NC}"
    echo -e "${BLUE}  HPC 클러스터 YAML 설정 파일 생성기${NC}"
    echo -e "${BLUE}=================================================${NC}"
    echo ""
    echo -e "입력 CSV: ${GREEN}$input_csv${NC}"
    echo -e "출력 YAML: ${GREEN}$output_yaml${NC}"
    echo ""

    # 배열 초기화
    declare -a controllers=()
    declare -a compute_nodes=()
    declare -a viz_nodes=()
    declare -a controller_ips=()

    local first_password=""
    local line_num=0

    # CSV 파일 읽기
    echo -e "${YELLOW}[1/3] CSV 파일 읽기 및 서버 정보 수집 중...${NC}"
    echo ""

    while IFS=',' read -r ip role ssh_user ssh_password || [ -n "$ip" ]; do
        line_num=$((line_num + 1))

        # 헤더 라인 건너뛰기
        if [ $line_num -eq 1 ]; then
            # 헤더인지 확인
            if [[ "$ip" == "ip" ]] || [[ "$ip" == "IP" ]]; then
                continue
            fi
        fi

        # 빈 줄 건너뛰기
        [ -z "$ip" ] && continue

        # 공백 제거
        ip=$(echo "$ip" | tr -d '[:space:]')
        role=$(echo "$role" | tr -d '[:space:]' | tr '[:upper:]' '[:lower:]')
        ssh_user=$(echo "$ssh_user" | tr -d '[:space:]')
        ssh_password=$(echo "$ssh_password" | tr -d '[:space:]')

        # 첫 번째 비밀번호 저장 (공통 비밀번호로 사용)
        [ -z "$first_password" ] && first_password="$ssh_password"

        echo -e "  ${BLUE}[*]${NC} $ip ($role) - 정보 수집 중..."

        # 서버 정보 조회
        local server_info
        server_info=$(get_server_info "$ip" "$ssh_user" "$ssh_password")

        # 정보 파싱
        IFS='|' read -r hostname cpus sockets cores_per_socket threads_per_core memory_mb disk_gb has_nvidia has_amd gpu_count <<< "$server_info"

        echo -e "    └─ hostname: ${GREEN}$hostname${NC}, CPUs: $cpus, Memory: ${memory_mb}MB"

        # 역할별 분류
        case "$role" in
            controller)
                controllers+=("$ip|$ssh_user|$hostname|$cpus|$memory_mb|$disk_gb")
                controller_ips+=("$ip")
                ;;
            compute)
                compute_nodes+=("$ip|$ssh_user|$hostname|$cpus|$sockets|$cores_per_socket|$threads_per_core|$memory_mb|$disk_gb")
                ;;
            viz)
                viz_nodes+=("$ip|$ssh_user|$hostname|$cpus|$sockets|$cores_per_socket|$threads_per_core|$memory_mb|$disk_gb|$has_nvidia|$has_amd|$gpu_count")
                ;;
            *)
                echo -e "  ${YELLOW}[WARN]${NC} 알 수 없는 역할: $role (건너뜀)"
                ;;
        esac

    done < "$input_csv"

    echo ""
    echo -e "${YELLOW}[2/3] YAML 파일 생성 중...${NC}"

    # 결과 요약
    echo ""
    echo -e "  Controllers: ${GREEN}${#controllers[@]}${NC}개"
    echo -e "  Compute Nodes: ${GREEN}${#compute_nodes[@]}${NC}개"
    echo -e "  Viz Nodes: ${GREEN}${#viz_nodes[@]}${NC}개"
    echo ""

    # YAML 헤더 생성
    generate_yaml_header "$output_yaml" "$first_password"

    # nodes 섹션 시작
    echo "# ============================================================================" >> "$output_yaml"
    echo "# Multi-Head Controllers (N중화 구성)" >> "$output_yaml"
    echo "# ============================================================================" >> "$output_yaml"
    echo "nodes:" >> "$output_yaml"

    # Controllers 생성
    if [ ${#controllers[@]} -gt 0 ]; then
        echo "  controllers:" >> "$output_yaml"
        local priority=100
        local is_first=true

        for controller in "${controllers[@]}"; do
            IFS='|' read -r ip ssh_user hostname cpus memory_mb disk_gb <<< "$controller"

            if [ "$is_first" = "true" ]; then
                generate_controller_yaml "$hostname" "$ip" "$ssh_user" "$cpus" "$memory_mb" "$disk_gb" "$priority" "true" >> "$output_yaml"
                is_first=false
            else
                generate_controller_yaml "$hostname" "$ip" "$ssh_user" "$cpus" "$memory_mb" "$disk_gb" "$priority" "false" >> "$output_yaml"
            fi
            priority=$((priority - 1))
        done
    fi

    # Compute 및 Viz 노드 생성
    echo "  # Compute Nodes" >> "$output_yaml"
    echo "  compute_nodes:" >> "$output_yaml"

    for node in "${compute_nodes[@]}"; do
        IFS='|' read -r ip ssh_user hostname cpus sockets cores_per_socket threads_per_core memory_mb disk_gb <<< "$node"
        generate_compute_yaml "$hostname" "$ip" "$ssh_user" "$cpus" "$sockets" "$cores_per_socket" "$threads_per_core" "$memory_mb" "$disk_gb" >> "$output_yaml"
    done

    for node in "${viz_nodes[@]}"; do
        IFS='|' read -r ip ssh_user hostname cpus sockets cores_per_socket threads_per_core memory_mb disk_gb has_nvidia has_amd gpu_count <<< "$node"
        generate_viz_yaml "$hostname" "$ip" "$ssh_user" "$cpus" "$sockets" "$cores_per_socket" "$threads_per_core" "$memory_mb" "$disk_gb" "$has_nvidia" "$has_amd" "$gpu_count" >> "$output_yaml"
    done

    # YAML 푸터 생성
    local compute_list=""
    local viz_list=""
    for node in "${compute_nodes[@]}"; do
        IFS='|' read -r ip _ hostname _ <<< "$node"
        compute_list+="$hostname\n"
    done
    for node in "${viz_nodes[@]}"; do
        IFS='|' read -r ip _ hostname _ <<< "$node"
        viz_list+="$hostname\n"
    done

    generate_yaml_footer "$output_yaml" "$(printf '%s\n' "${controller_ips[@]}")" "$compute_list" "$viz_list"

    # 파일 권한 설정
    chmod 600 "$output_yaml"

    echo ""
    echo -e "${YELLOW}[3/3] 완료!${NC}"
    echo ""
    echo -e "${GREEN}=================================================${NC}"
    echo -e "${GREEN}  YAML 파일이 성공적으로 생성되었습니다!${NC}"
    echo -e "${GREEN}=================================================${NC}"
    echo ""
    echo -e "출력 파일: ${BLUE}$output_yaml${NC}"
    echo -e "파일 권한: ${BLUE}600 (소유자만 읽기/쓰기)${NC}"
    echo ""
    echo -e "${YELLOW}다음 단계:${NC}"
    echo "  1. 생성된 YAML 파일을 검토하고 필요시 수정하세요"
    echo "  2. environment 섹션의 비밀번호를 확인하세요"
    echo "  3. sudo ./setup_cluster_full_multihead.sh 를 실행하세요"
    echo ""
}

# 인자 확인
if [ $# -lt 1 ]; then
    usage
fi

main "$@"
